# ğŸŸ Predicting Fish Weight Using Machine Learning

This project builds an **end-to-end ML pipeline** to accurately predict fish weight based on morphological features such as length, height, width, and species.  
It evaluates multiple **regression models** and identifies the best-performing one using comprehensive performance metrics.

---

## ğŸ“Œ Table of Contents
- [Introduction](#-introduction)
- [Problem Statement](#-problem-statement)
- [Dataset Description](#-dataset-description)
- [Exploratory Data Analysis](#-exploratory-data-analysis)
- [Preprocessing](#-preprocessing)
- [Model Training and Evaluation](#-model-training-and-evaluation)
- [Best Performing Model](#-best-performing-model)
- [Visualizations](#-visualizations)
- [Hyperparameter Tuning](#-hyperparameter-tuning)
- [Conclusion](#-conclusion)

---

## ğŸ§¾ Introduction
Accurately predicting the **weight of a fish** based on physical attributes is crucial in aquaculture, fisheries management, and food logistics.  
This project demonstrates an ML workflow to estimate fish weight and compares regression models for effectiveness.

---

## ğŸ¯ Problem Statement
Given the physical measurements of a fish (**lengths, height, width, and species**), predict its **weight**.  
This is formulated as a **supervised regression problem**.

---

## ğŸ“‚ Dataset Description
- Dataset: **Fish Market Dataset** (from Kaggle)  
- Features:
  - `Species` (categorical)
  - `Weight` (target)
  - `Length1`, `Length2`, `Length3`
  - `Height`
  - `Width`

ğŸ“Œ **Feature Engineering**  
A new feature was created:  
```python
Volume_Estimate = Length1 * Height * Width
```

## ğŸ” Exploratory Data Analysis (EDA)
- Found strong correlations between **Weight** and both **Height** and **Volume_Estimate**.  
- Removed **outliers** (e.g., `Weight = 0` and mislabeled samples).  
- Scatterplots and heatmaps confirmed **linear trends** between features and target.  

---

## âš™ï¸ Preprocessing
Steps applied:  
- ğŸ—‘ï¸ Dropped invalid/outlier entries  
- ğŸ“ Engineered `Volume_Estimate`  
- ğŸ“Š Standard scaling for numerical features  
- ğŸ”¤ One-hot encoding for `Species`  
- âœ‚ï¸ 80/20 **Train-Test split**  

---

## ğŸ¤– Model Training and Evaluation
Trained models:  
- Linear Regression  
- Decision Tree Regressor  
- Random Forest Regressor  
- Gradient Boosting Regressor  

### ğŸ“Š Evaluation Metrics
- Train/Test **RMSE**  
- Test **MAE**  
- Test **RÂ² Score**  
- Cross-validated **RÂ² Mean & Std**  

| Model              | Train RMSE | Test RMSE | Test MAE | Test RÂ²  | CV RÂ² Mean | CV RÂ² Std |
|--------------------|------------|-----------|----------|----------|------------|-----------|
| **Linear Regression** | 46.08      | 40.89     | 28.64    | **0.9850** | 0.9666     | 0.0119    |
| Gradient Boosting  | 6.66       | 48.65     | 34.19    | 0.9788   | 0.9668     | 0.0185    |
| Random Forest      | 19.59      | 52.37     | 32.98    | 0.9755   | 0.9658     | 0.0232    |
| Decision Tree      | 0.00       | 68.37     | 42.45    | 0.9582   | 0.9126     | 0.0434    |

---

## ğŸ† Best Performing Model
âœ… **Linear Regression**  
- Achieved the highest **RÂ² = 0.9850** on the test set  
- Outperformed ensemble models despite simplicity  
- Generalized well â†’ close **Train/Test RMSE** values  
- Strong **linear correlation** between features and target  

---

## ğŸ“Š Visualizations
Generated plots:  
- ğŸ“Œ Actual vs. Predicted scatter plots  
- ğŸ“‰ Residual plots  
- ğŸ“Š Error distribution histograms  
- ğŸ“ˆ Learning curves  
- ğŸª„ Feature importance bar plots  

ğŸ” **Linear Regression** had the most **stable residuals** and lowest variance.  

---

## ğŸ”§ Hyperparameter Tuning
- Applied **GridSearchCV** for Random Forest and Gradient Boosting  
- Slight improvement in RÂ² but **did not surpass Linear Regression**  

---

## âœ… Conclusion
- **Linear Regression** proved most effective due to strong **linear structure** in data  
- Feature engineering (`Volume_Estimate`) greatly enhanced accuracy  
- Complex models (RF, GBM) did not outperform simpler models  

---

## ğŸ”® Future Work
- ğŸŸ Collect more **diverse fish datasets**  
- ğŸ“ˆ Explore **non-linear models** if dataset grows in complexity  
- ğŸ¤– Consider **neural networks or transformers** for large-scale datasets  
- ğŸŒ Apply in **real-time aquaculture systems** for operational use  
